{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Solar Mini Chat on SageMaker JumpStart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Solar Mini Chat** is an advanced English/Korean large language model developed by Upstage. Specifically fine-tuned for multi-turn chat purposes, it demonstrates enhanced performance across a wide range of natural language processing tasks. This fine-tuning equips it with the ability to handle extended conversations more effectively, making it particularly adept for interactive applications. It employs a scaling method called ‘depth up-scaling’ (DUS), which is comprised of depthwise scaling and continued pretraining. DUS allows for a much more straightforward and efficient enlargement of smaller models than other scaling methods such as mixture-of-experts. \n",
    "\n",
    "This sample notebook shows you how to deploy [Solar Mini Chat](https://aws.amazon.com/marketplace/...) using Amazon SageMaker. (TODO: Fix the link)\n",
    "\n",
    "## Pre-requisites:\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "\n",
    "## Contents:\n",
    "1. [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Create an endpoint and perform real-time inference](#2.-Create-an-endpoint-and-perform-real-time-inference)\n",
    "   1. [Create an endpoint](#A.-Create-an-endpoint)\n",
    "   2. [Prepare input payload](#B.-Prepare-input-payload)\n",
    "   3. [Perform real-time inference](#C.-Perform-real-time-inference)\n",
    "3. [Clean-up](#4.-Clean-up)\n",
    "   1. [Delete the endpoint](#A.-Delete-the-endpoint)\n",
    "   2. [Delete the model](#B.-Delete-the-model)\n",
    "    \n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to the model package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To subscribe to the model package:\n",
    "1. Open the model package [listing page](https://aws.amazon.com/marketplace/pp/...) (TODO: Update link)\n",
    "2. On the AWS Marketplace listing, click on the **Continue to subscribe** button.\n",
    "3. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agrees with EULA, pricing, and support terms. \n",
    "4. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model using Boto3. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We offer two types of packages. (TODO: Update ARNs)\n",
    "* Solar-1-Mini-Chat (Support `ml.g5.12xlarge`)\n",
    "    * `arn:aws:sagemaker:us-west-2::model-package/Solar-1-Mini-Chat`\n",
    "* 4-bit quantized version of Solar-1-Mini-Chat (Support `ml.g5.2xlarge`)\n",
    "    * `arn:aws:sagemaker:us-west-2::model-package/Solar-1-Mini-Chat-4bit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Package: 'arn:aws:sagemaker:us-west-2::model-package/Solar-1-Mini-Chat-4bit'\n"
     ]
    }
   ],
   "source": [
    "# Choose one of our model packages\n",
    "model_package_arn = (\n",
    "    # \"arn:aws:sagemaker:us-west-2::model-package/Solar-1-Mini-Chat\"\n",
    "    \"arn:aws:sagemaker:us-west-2::model-package/Solar-1-Mini-Chat-4bit\"\n",
    ")\n",
    "print(f\"Model Package: '{model_package_arn}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import ModelPackage, get_execution_role, serializers, deserializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an endpoint and perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to understand how real-time inference with Amazon SageMaker works, see [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"solar-1-mini-chat\"\n",
    "content_type = \"application/json\"\n",
    "\n",
    "# ml.g5.2xlarge for quantized version, or\n",
    "# ml.g5.12xlarge for non-quantized version\n",
    "real_time_inference_instance_type = (\n",
    "    \"ml.g5.2xlarge\"\n",
    "    # \"ml.g5.12xlarge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint name: 'solar-1-mini-chat-2024-02-06-06-07-59-503'\n"
     ]
    }
   ],
   "source": [
    "# create a deployable model from the model package.\n",
    "model = ModelPackage(\n",
    "    role=role, model_package_arn=model_package_arn, sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "endpoint_name = sagemaker.utils.name_from_base(model_name)\n",
    "print(f\"endpoint name: '{endpoint_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "# Deploy the model\n",
    "model.deploy(1, real_time_inference_instance_type, endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# our requests and responses will be in json format,\n",
    "# so we specify the serializer and the deserializer\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint has been created, you would be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Prepare input payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We support request/response payload compitable to OpenAI's Chat completion endpoint.\n",
    "\n",
    "Supported parameters:\n",
    "- messages(list of objects)*: List of messages that contains `role` and `content`. `role` must be one of [`system`, `user`, `assistant`].\n",
    "- model(string): You can use `model` parameter for compitability, but since We have only one model, this parameter is not required.\n",
    "- frequency_penalty(number): Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n",
    "- presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n",
    "- max_tokens: The maximum number of tokens that can be generated in the chat completion. Solar support maximum 4k(4096) context for input and generated tokens.\n",
    "- temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n",
    "- top_p: An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n",
    "\n",
    "**required*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Single-turn chat example\n",
    "input = {\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant.\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Can you provide a Python script to merge two sorted lists?\"\n",
    "      }\n",
    "    ],\n",
    "    \"temperature\": 0.7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To limit length of your output, you can use max_tokens parameter.\n",
    "input = {\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant.\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Can you provide a Python script to merge two sorted lists?\"\n",
    "      }\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Multi-turn chat example\n",
    "input = {\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant.\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Can you provide a Python script to merge two sorted lists?\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"\"\"Sure, here is a Python script to merge two sorted lists:\n",
    "\n",
    "                    ```python\n",
    "                    def merge_lists(list1, list2):\n",
    "                        return sorted(list1 + list2)\n",
    "                    ```\n",
    "                    \"\"\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Can you provide an example of how to use this function?\"\n",
    "      }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# real-time inference\n",
    "response = predictor.predict(\n",
    "    input\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response example:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"id\": \"cmpl-cc3f71b58086441fa6a53480b94fd0ec\",\n",
    "    \"object\": \"chat.completion\",\n",
    "    \"created\": 3421,\n",
    "    \"model\": \"/opt/ml/model\",\n",
    "    \"choices\": [\n",
    "        {\n",
    "            \"index\": 0,\n",
    "            \"message\": {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"What do you call a fake noodle?\\n\\nAn impasta!\"\n",
    "            },\n",
    "            \"finish_reason\": \"stop\"\n",
    "        }\n",
    "    ],\n",
    "    \"usage\": {\n",
    "        \"prompt_tokens\": 34,\n",
    "        \"total_tokens\": 334,\n",
    "        \"completion_tokens\": 300\n",
    "    }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"cmpl-514c9a7f9ef449a0b4ba1fd950715622\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 4207,\n",
      "    \"model\": \"/opt/ml/model\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"index\": 0,\n",
      "            \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"Sure, here's a simple Python script to merge two sorted lists:\\n\\n```python\\ndef merge_sorted_lists(list1, list2):\\n    result = []\\n    while list1 and list2:\\n        if list1[0] < list2[0]:\\n            result.append(list1.pop(0))\\n        else:\\n            result.append(list2.pop(0))\\n    result += list1\\n    result += list2\\n    return result\\n\\n# Test the function\\nlist1 = [1, 3, 5]\\nlist2 = [2, 4, 6]\\nprint(merge_sorted_lists(list1, list2))  # Output: [1, 2, 3, 4, 5, 6]\\n```\\n\\nThis script defines a function called `merge_sorted_lists` that takes two lists as input and returns a new list containing the elements of both input lists in sorted order. The function uses a while loop to compare the first elements of the input lists and append the smaller one to the result list, then pops that element from the corresponding input list. The loop continues until one of the input lists is empty, at which point the function appends the remaining elements from the non-empty input list to the result list and returns the result.\"\n",
      "            },\n",
      "            \"finish_reason\": \"stop\"\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 34,\n",
      "        \"total_tokens\": 329,\n",
      "        \"completion_tokens\": 295\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Since We use `JSONDeserializer`, output of `preidct` is dictionary object\n",
    "print(json.dumps(response, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: \n",
      "Sure, here's a simple Python script to merge two sorted lists:\n",
      "\n",
      "```python\n",
      "def merge_sorted_lists(list1, list2):\n",
      "    result = []\n",
      "    while list1 and list2:\n",
      "        if list1[0] < list2[0]:\n",
      "            result.append(list1.pop(0))\n",
      "        else:\n",
      "            result.append(list2.pop(0))\n",
      "    result += list1\n",
      "    result += list2\n",
      "    return result\n",
      "\n",
      "# Test the function\n",
      "list1 = [1, 3, 5]\n",
      "list2 = [2, 4, 6]\n",
      "print(merge_sorted_lists(list1, list2))  # Output: [1, 2, 3, 4, 5, 6]\n",
      "```\n",
      "\n",
      "This script defines a function called `merge_sorted_lists` that takes two lists as input and returns a new list containing the elements of both input lists in sorted order. The function uses a while loop to compare the first elements of the input lists and append the smaller one to the result list, then pops that element from the corresponding input list. The loop continues until one of the input lists is empty, at which point the function appends the remaining elements from the non-empty input list to the result list and returns the result.\n"
     ]
    }
   ],
   "source": [
    "print(\"assistant: \")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you can delete the endpoint and avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(endpoint_name)\n",
    "model.sagemaker_session.delete_endpoint_config(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
